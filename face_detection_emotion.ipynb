{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face detection emotion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3LqHzskt5zc55EiI6J4FL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Premjit1/face-detection-emotion/blob/main/face_detection_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aWjoOL7jUTsm"
      },
      "outputs": [],
      "source": [
        "import sys, os  \n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "  \n",
        "from keras.models import Sequential  \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten  \n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D  \n",
        "from keras.losses import categorical_crossentropy  \n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        "from keras.regularizers import l2  \n",
        "from keras.utils import np_utils  \n",
        "# pd.set_option('display.max_rows', 500)  \n",
        "# pd.set_option('display.max_columns', 500)  \n",
        "# pd.set_option('display.width', 1000)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbZcVB3UVkuO",
        "outputId": "1095ea3e-e156-4740-e3e9-a25ae5211cc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/fer2013.csv'"
      ],
      "metadata": {
        "id": "fb3HJEBkVz9i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "sc2KcPrHWshU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info()) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q17BG2DDW4ja",
        "outputId": "41a0dec9-ed72-4f32-f3cc-187de8b59ea9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1   pixels   35887 non-null  object\n",
            " 2   Usage    35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Usage\"].value_counts())  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpqjViL3W9V4",
        "outputId": "308888e5-627f-4d21-d032-10da7443c800"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: Usage, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,train_y,X_test,test_y=[],[],[],[]  \n",
        "  \n",
        "for index, row in df.iterrows():  \n",
        "    val=row['pixels'].split(\" \")  \n",
        "    try:  \n",
        "        if 'Training' in row['Usage']:  \n",
        "           X_train.append(np.array(val,'float32'))  \n",
        "           train_y.append(row['emotion'])  \n",
        "        elif 'PublicTest' in row['Usage']:  \n",
        "           X_test.append(np.array(val,'float32'))  \n",
        "           test_y.append(row['emotion'])  \n",
        "    except:  \n",
        "        print(f\"error occured at index :{index} and row:{row}\")  "
      ],
      "metadata": {
        "id": "zTyGSEALXCLB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 64  \n",
        "num_labels = 7  \n",
        "batch_size = 64  \n",
        "epochs = 30  \n",
        "width, height = 48, 48  \n",
        "  "
      ],
      "metadata": {
        "id": "Anstdu84XNjM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train,'float32')  \n",
        "train_y = np.array(train_y,'float32')  \n",
        "X_test = np.array(X_test,'float32')  \n",
        "test_y = np.array(test_y,'float32')  "
      ],
      "metadata": {
        "id": "6Rs-qJtxXTD2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)  \n",
        "test_y=np_utils.to_categorical(test_y, num_classes=num_labels)  "
      ],
      "metadata": {
        "id": "ySHP694vXXES"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalizing data between oand 1  \n",
        "X_train -= np.mean(X_train, axis=0)  \n",
        "X_train /= np.std(X_train, axis=0) "
      ],
      "metadata": {
        "id": "ucmXQeQmXcvk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test -= np.mean(X_test, axis=0)  \n",
        "X_test /= np.std(X_test, axis=0)"
      ],
      "metadata": {
        "id": "LFveji1EXpHc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)  \n",
        "  \n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)  "
      ],
      "metadata": {
        "id": "qKhZzcNbXtPA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"shape:{X_train.shape}\")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PrxY1FvXx7Q",
        "outputId": "e26f9bcb-26d2-49a5-cb91-67376d2b9329"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape:(28709, 48, 48, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##designing the cnn  \n",
        "#1st convolution layer  \n",
        "model = Sequential()  "
      ],
      "metadata": {
        "id": "E7X0fMnAX3uP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))  \n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5)) "
      ],
      "metadata": {
        "id": "-n07i4VZX8Ty"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2nd convolution layer  \n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5)) "
      ],
      "metadata": {
        "id": "GgAoBfTPYA2R"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3rd convolution layer  \n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "  \n",
        "model.add(Flatten()) "
      ],
      "metadata": {
        "id": "3UNaGeGIYFEy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fully connected neural networks  \n",
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  \n",
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  "
      ],
      "metadata": {
        "id": "hcfhCPIaYJIO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(num_labels, activation='softmax'))  "
      ],
      "metadata": {
        "id": "3iDqAVJ0YNwE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()  \n",
        "  \n",
        "#Compliling the model  \n",
        "model.compile(loss=categorical_crossentropy,  \n",
        "              optimizer=Adam(),  \n",
        "              metrics=['accuracy']) "
      ],
      "metadata": {
        "id": "W6nDAKYKYSkj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model  \n",
        "model.fit(X_train, train_y,  \n",
        "          batch_size=batch_size,  \n",
        "          epochs=epochs,  \n",
        "          verbose=1,  \n",
        "          validation_data=(X_test, test_y),  \n",
        "          shuffle=True)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yry7l1a_YYb4",
        "outputId": "aeeb8a2e-d40f-4ac9-f2d3-5a6019b98572"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "449/449 [==============================] - 525s 1s/step - loss: 1.7273 - accuracy: 0.2913 - val_loss: 1.5821 - val_accuracy: 0.3948\n",
            "Epoch 2/30\n",
            "449/449 [==============================] - 523s 1s/step - loss: 1.5433 - accuracy: 0.3903 - val_loss: 1.4008 - val_accuracy: 0.4642\n",
            "Epoch 3/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 1.4350 - accuracy: 0.4461 - val_loss: 1.3453 - val_accuracy: 0.4815\n",
            "Epoch 4/30\n",
            "449/449 [==============================] - 525s 1s/step - loss: 1.3621 - accuracy: 0.4728 - val_loss: 1.2839 - val_accuracy: 0.5079\n",
            "Epoch 5/30\n",
            "449/449 [==============================] - 526s 1s/step - loss: 1.3155 - accuracy: 0.4917 - val_loss: 1.2936 - val_accuracy: 0.5085\n",
            "Epoch 6/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 1.2811 - accuracy: 0.5086 - val_loss: 1.2523 - val_accuracy: 0.5113\n",
            "Epoch 7/30\n",
            "449/449 [==============================] - 525s 1s/step - loss: 1.2426 - accuracy: 0.5201 - val_loss: 1.2249 - val_accuracy: 0.5294\n",
            "Epoch 8/30\n",
            "449/449 [==============================] - 526s 1s/step - loss: 1.2158 - accuracy: 0.5313 - val_loss: 1.2035 - val_accuracy: 0.5311\n",
            "Epoch 9/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 1.1964 - accuracy: 0.5410 - val_loss: 1.1940 - val_accuracy: 0.5444\n",
            "Epoch 10/30\n",
            "449/449 [==============================] - 522s 1s/step - loss: 1.1655 - accuracy: 0.5503 - val_loss: 1.1984 - val_accuracy: 0.5447\n",
            "Epoch 11/30\n",
            "449/449 [==============================] - 521s 1s/step - loss: 1.1513 - accuracy: 0.5613 - val_loss: 1.1720 - val_accuracy: 0.5492\n",
            "Epoch 12/30\n",
            "449/449 [==============================] - 523s 1s/step - loss: 1.1353 - accuracy: 0.5664 - val_loss: 1.1600 - val_accuracy: 0.5595\n",
            "Epoch 13/30\n",
            "449/449 [==============================] - 522s 1s/step - loss: 1.1220 - accuracy: 0.5690 - val_loss: 1.1670 - val_accuracy: 0.5500\n",
            "Epoch 14/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 1.0993 - accuracy: 0.5820 - val_loss: 1.1574 - val_accuracy: 0.5570\n",
            "Epoch 15/30\n",
            "449/449 [==============================] - 523s 1s/step - loss: 1.0871 - accuracy: 0.5848 - val_loss: 1.1517 - val_accuracy: 0.5631\n",
            "Epoch 16/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 1.0636 - accuracy: 0.5970 - val_loss: 1.1667 - val_accuracy: 0.5528\n",
            "Epoch 17/30\n",
            "449/449 [==============================] - 525s 1s/step - loss: 1.0606 - accuracy: 0.5989 - val_loss: 1.1668 - val_accuracy: 0.5581\n",
            "Epoch 18/30\n",
            "449/449 [==============================] - 523s 1s/step - loss: 1.0385 - accuracy: 0.6062 - val_loss: 1.1370 - val_accuracy: 0.5762\n",
            "Epoch 19/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 1.0329 - accuracy: 0.6083 - val_loss: 1.1478 - val_accuracy: 0.5676\n",
            "Epoch 20/30\n",
            "449/449 [==============================] - 525s 1s/step - loss: 1.0105 - accuracy: 0.6127 - val_loss: 1.1773 - val_accuracy: 0.5550\n",
            "Epoch 21/30\n",
            "449/449 [==============================] - 525s 1s/step - loss: 1.0056 - accuracy: 0.6150 - val_loss: 1.1788 - val_accuracy: 0.5670\n",
            "Epoch 22/30\n",
            "449/449 [==============================] - 522s 1s/step - loss: 0.9814 - accuracy: 0.6228 - val_loss: 1.1736 - val_accuracy: 0.5609\n",
            "Epoch 23/30\n",
            "449/449 [==============================] - 522s 1s/step - loss: 0.9658 - accuracy: 0.6327 - val_loss: 1.1622 - val_accuracy: 0.5701\n",
            "Epoch 24/30\n",
            "449/449 [==============================] - 526s 1s/step - loss: 0.9648 - accuracy: 0.6321 - val_loss: 1.1689 - val_accuracy: 0.5740\n",
            "Epoch 25/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 0.9528 - accuracy: 0.6377 - val_loss: 1.1796 - val_accuracy: 0.5726\n",
            "Epoch 26/30\n",
            "449/449 [==============================] - 522s 1s/step - loss: 0.9419 - accuracy: 0.6376 - val_loss: 1.1776 - val_accuracy: 0.5782\n",
            "Epoch 27/30\n",
            "449/449 [==============================] - 524s 1s/step - loss: 0.9215 - accuracy: 0.6468 - val_loss: 1.1997 - val_accuracy: 0.5756\n",
            "Epoch 28/30\n",
            "449/449 [==============================] - 526s 1s/step - loss: 0.9124 - accuracy: 0.6568 - val_loss: 1.1698 - val_accuracy: 0.5759\n",
            "Epoch 29/30\n",
            "449/449 [==============================] - 525s 1s/step - loss: 0.9083 - accuracy: 0.6559 - val_loss: 1.1972 - val_accuracy: 0.5723\n",
            "Epoch 30/30\n",
            "449/449 [==============================] - 526s 1s/step - loss: 0.8937 - accuracy: 0.6595 - val_loss: 1.1842 - val_accuracy: 0.5720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3c2a49750>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the  model to  use it later on  \n",
        "fer_json = model.to_json()  \n",
        "with open(\"fer.json\", \"w\") as json_file:  \n",
        "    json_file.write(fer_json)  \n",
        "model.save_weights(\"fer.h5\")    "
      ],
      "metadata": {
        "id": "r7KCbu9hPlT2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import model_from_json"
      ],
      "metadata": {
        "id": "kvlItP40ooD7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "  "
      ],
      "metadata": {
        "id": "-Ub6eZrEcfNG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  \n",
        "import cv2  \n",
        "import numpy as np  \n",
        "from keras.models import model_from_json  \n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "from keras.preprocessing import image  \n",
        "# load json and create model\n",
        "json_file = open('fer.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "emotion_model = model_from_json(loaded_model_json)\n",
        "emotion_model.load_weights(\"fer.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "cap = cv2.VideoCapture()\n",
        "while True:  \n",
        "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
        "    if not ret:  \n",
        "        break \n",
        "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
        "  \n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "    for (x,y,w,h) in faces_detected:  \n",
        "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
        "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
        "        img_pixels = image.img_to_array(roi_gray)  \n",
        "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "        img_pixels /= 255 \n",
        "        predictions = model.predict(img_pixels)  \n",
        "        #find max indexed array  \n",
        "        max_index = np.argmax(predictions[0])  \n",
        "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
        "        predicted_emotion = emotions[max_index]  \n",
        "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
        "        resized_img = cv2.resize(test_img, (1000, 700))  \n",
        "        cv2.imshow('Facial emotion analysis ',resized_img)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "         break\n",
        "        cv2.waitKey()\n",
        "        cv2.destroyAllWindows() \n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxkEja4ySZuA",
        "outputId": "5935f81c-6399-404b-9d30-e49bcee83055"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  \n",
        "import cv2  \n",
        "import numpy as np  \n",
        "from keras.models import model_from_json  \n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "from keras.preprocessing import image  \n",
        "# load json and create model\n",
        "json_file = open('fer.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "emotion_model = model_from_json(loaded_model_json)\n",
        "emotion_model.load_weights(\"fer.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "cap = cv2.VideoCapture()\n",
        "while True:  \n",
        "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
        "    if not ret:  \n",
        "        break \n",
        "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
        "  \n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "    for (x,y,w,h) in faces_detected:  \n",
        "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
        "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
        "        img_pixels = image.img_to_array(roi_gray)  \n",
        "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "        img_pixels /= 255 \n",
        "        predictions = model.predict(img_pixels)  \n",
        "        #find max indexed array  \n",
        "        max_index = np.argmax(predictions[0])  \n",
        "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
        "        predicted_emotion = emotions[max_index]  \n",
        "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
        "        resized_img = cv2.resize(test_img, (1000, 700))  \n",
        "        cv2.imshow('Facial emotion analysis ',resized_img)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "         break\n",
        "        cv2.waitKey()\n",
        "        cv2.destroyAllWindows() \n",
        "        "
      ],
      "metadata": {
        "id": "T6FZ-EXqz32S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcprg2sWgHhS",
        "outputId": "0f5cf624-191e-49ac-ab64-c18daa75dfd7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.9 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 52.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 45.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 41.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 59.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 130 kB 52.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 793 kB 47.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 380 kB 58.9 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.2 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pOWOd3LhmB-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "YUQK9r2EmEHk"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit_webrtc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9xrF07dmHV7",
        "outputId": "2cdb0713-6b16-4baf-aa7c-e552fabee771"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 856 kB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 49.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 36.7 MB 7.6 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 50.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 71 kB 8.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 269 kB 54.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app2.py\n",
        "import os  \n",
        "import cv2  \n",
        "import numpy as np  \n",
        "from keras.models import model_from_json  \n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "from keras.preprocessing import image  \n",
        "# load json and create model\n",
        "json_file = open('fer.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "emotion_model = model_from_json(loaded_model_json)\n",
        "emotion_model.load_weights(\"fer.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "cap = cv2.VideoCapture()\n",
        "while True:  \n",
        "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
        "    if not ret:  \n",
        "        break \n",
        "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
        "  \n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
        "    for (x,y,w,h) in faces_detected:  \n",
        "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
        "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
        "        img_pixels = image.img_to_array(roi_gray)  \n",
        "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "        img_pixels /= 255 \n",
        "        predictions = model.predict(img_pixels)  \n",
        "        #find max indexed array  \n",
        "        max_index = np.argmax(predictions[0])  \n",
        "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
        "        predicted_emotion = emotions[max_index]  \n",
        "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
        "        resized_img = cv2.resize(test_img, (1000, 700))  \n",
        "        cv2.imshow('Facial emotion analysis ',resized_img)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "         break\n",
        "        cv2.waitKey()\n",
        "        cv2.destroyAllWindows() \n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNEDsG_k0MGK",
        "outputId": "2a68cda4-751b-42d3-ceea-3a170bfbbf94"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app2.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3uZ4DzM0h9J",
        "outputId": "9f7c8409-cecb-4eeb-b55b-64816495623b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-23 17:29:57.536 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[K\u001b[?25h\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.80.22.203:8501\u001b[0m\n",
            "\u001b[0m\n",
            "npx: installed 22 in 3.19s\n",
            "your url is: https://yellow-donkey-19.loca.lt\n",
            "2022-03-23 17:32:02.996778: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n"
          ]
        }
      ]
    }
  ]
}